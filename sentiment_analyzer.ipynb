{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Creates a sound when training is done\n",
    "## Import up sound alert dependencies\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "def allDone():\n",
    "  display(Audio(url='https://sound.peal.io/ps/audios/000/000/537/original/woo_vu_luvub_dub_dub.wav', autoplay=True))\n",
    "## Insert whatever audio file you want above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "virtual environment 'sentiment', virtual env kernel active\n",
    "\n",
    "    conda install -n sent.analyzer spacy\n",
    "    python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing\n",
    "Tokenization is the process of breaking down chunks of text into smaller pieces. spaCy comes with a default processing pipeline that begins with tokenization, making this process a snap. In spaCy, you can do either sentence tokenization or word tokenization:\n",
    "\n",
    "    Word tokenization breaks text down into individual words.\n",
    "    Sentence tokenization breaks text down into individual sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "Dave watched as the forest burned up on the hill,\n",
    "only a few miles from his house. The car had\n",
    "been hastily packed and Marta was inside trying to round\n",
    "up the last of the pets. \"Where could she be?\" he wondered\n",
    "as he continued to wait for Marta to appear with the pets.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_list = [token for token in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_list;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_tokens = [token for token in doc if not token.is_stop]\n",
    "filtered_tokens;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmas = [f'Token: {token}, lemma: {token.lemma_}' for token in filtered_tokens]\n",
    "lemmas;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Your Own NLP Sentiment Analyzer\n",
    "From the previous sections, you’ve probably noticed four major stages of building a sentiment analysis pipeline:\n",
    "\n",
    "    Loading data\n",
    "    Preprocessing\n",
    "    Training the classifier\n",
    "    Classifying data\n",
    "    \n",
    "For building a real-life sentiment analyzer, you’ll work through each of the steps that compose these stages. You’ll use the Large Movie Review Dataset compiled by Andrew Maas to train and test your sentiment analyzer. Once you’re ready, proceed to the next section to load your data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and Preprocessing Data"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def load_training_data(\n",
    "    data_directory: str = \"aclImdb/train\",\n",
    "    split: float = 0.8,\n",
    "    limit: int = 0\n",
    ") -> tuple:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to iterate through all the files in the dataset and load them into a list"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import os\n",
    "\n",
    "def load_training_data(data_directory: str = \"aclImdb/train\",split: float = 0.8,limit: int = 0) -> tuple:\n",
    "    \"\"\"\n",
    "    Split (float) is the proportion of data used to train, remainder tests\n",
    "    \"\"\"\n",
    "    #load from files\n",
    "    reviews = []\n",
    "    for label in [\"pos\", \"neg\"]:\n",
    "        labeled_directory = f\"{data_directory}/{label}\"\n",
    "        for review in os.listdir(labeled_directory):\n",
    "            if review.endswith(\".txt\"):\n",
    "                with open(f\"{labeled_directory}/{review}\") as f:\n",
    "                          text = f.read()\n",
    "                          text = text.replace(\"<br />\", \"\\n\\n\")\n",
    "                          if text.strip():\n",
    "                              spacy_label = {\n",
    "                                  \"cats\": {\n",
    "                                      \"pos\": \"pos\" == label,\n",
    "                                      \"neg\": \"neg\" == label\n",
    "                                  }}\n",
    "                              reviews.append((text,spacy_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Below randomly shuffle the order of the reviews to reduce the possible bias produced from loading order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "\n",
    "def load_training_data(data_directory: str = \"aclImdb/train\",split: float = 0.8,limit: int = 0) -> tuple:\n",
    "    \"\"\"\n",
    "    Split (float) is the proportion of data used to train, remainder tests\n",
    "    \"\"\"\n",
    "    #load from files\n",
    "    reviews = []\n",
    "    for label in [\"pos\", \"neg\"]:\n",
    "        labeled_directory = f\"{data_directory}/{label}\"\n",
    "        for review in os.listdir(labeled_directory):\n",
    "            if review.endswith(\".txt\"):\n",
    "                with open(f\"{labeled_directory}/{review}\",encoding = \"utf8\") as f:\n",
    "                          text = f.read()\n",
    "                          text = text.replace(\"<br />\", \"\\n\\n\")\n",
    "                          if text.strip():\n",
    "                              spacy_label = {\n",
    "                                  \"cats\": {\n",
    "                                      \"pos\": \"pos\" == label,\n",
    "                                      \"neg\": \"neg\" == label\n",
    "                                  }}\n",
    "                              reviews.append((text,spacy_label))\n",
    "    random.shuffle(reviews)\n",
    "    \n",
    "    if limit:\n",
    "        reviews = reviews[:limit]\n",
    "    split = int(len(reviews) * split)\n",
    "    return reviews[:split], reviews[split:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Your Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting the spaCy pipeline together allows you to rapidly build and train a convolutional neural network (CNN) for classifying text data. \n",
    "    \n",
    "    1. Modifying the base spaCy pipeline to include the textcat component\n",
    "    2. Building a training loop to train the textcat component\n",
    "    3. Evaluating the progress of your model training after a given number of training loops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Pipeline"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def train_model(training_data: list, test_data: list, iterations: int = 20) -> None:\n",
    "    #Build Pipline\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    if \"textcat\" not in nlp.pipe_names:\n",
    "        textcat = nlp.create_pipe(\n",
    "            \"textcat\", config = {\"architecture\": \"simple_cnn\"}\n",
    "        )\n",
    "        nlp.add_pipe(textcat, last = True)\n",
    "    else:\n",
    "        textcat = nlp.get_pipe(\"textcat\")\n",
    "    \n",
    "    textcat.add_label(\"pos\")\n",
    "    textcat.add_label(\"neg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Your Training Loop to Train textcat"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import os\n",
    "import random\n",
    "import spacy\n",
    "from spacy.util import minibatch, compounding"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def train_model(training_data: list, test_data: list, iterations: int= 20) -> None:\n",
    "    #Build Pipeline\n",
    "    nlp = spacy.load('en_core_web_sm') # load the english model\n",
    "    if \"textcat\" not in nlp.pipe_names:\n",
    "        textcat = nlp.create_pipe(\"textcat\", config = {\"architecture\": \"simple_cnn\"})\n",
    "        nlp.add_pipe(textcat,last=True)\n",
    "    else:\n",
    "        textcat = nlp.get_pipe('textcat')\n",
    "    \n",
    "    textcat.add_label(\"pos\")\n",
    "    textcat.add_label('neg')\n",
    "    \n",
    "    #Train only textcat\n",
    "    training_excluded_pipes = [pipe for pip3 in nlp.pipe_names if pipe != \"textcat\"]\n",
    "    \n",
    "    with nlp.disable_pips(training_excluded_pipes):\n",
    "        optimizer = nlp.begin_training()\n",
    "        # Training loop\n",
    "        print(\"Beginning Training\")\n",
    "        print(\"Loss\\tPrecision\\tRecall\\tF-score\")\n",
    "        batch_sizes = compounding(4.0,32.0,1.001) # a generator that yields infinite series of input numbers\n",
    "        \n",
    "        for i in range(iterations):\n",
    "            loss = {}\n",
    "            random.shuffle(training_data)\n",
    "            batches = minibatch(training_data,size = batch_sizes)\n",
    "            for batch in batches:\n",
    "                text, labels = zip(*batch)\n",
    "                nlp.update(\n",
    "                    text,\n",
    "                    labels,\n",
    "                    drop = 0.2,\n",
    "                    sgd = optimizer,\n",
    "                    losses = loss\n",
    "                )\n",
    "            with textcat.model.use_params(optimizer.averages):\n",
    "                evaluations_results = evaluate_model(tokenizer = nlp.tokenizer,textcat=textcat,test_data = test_data) #evaluate model function\n",
    "                \n",
    "                print(\n",
    "                    f\"{loss['textcat']}\\t{evaluation_results['precision']}\"\n",
    "                    f\"\\t{evaluation_results['recall']}\"\n",
    "                    f\"\\t{evaluation_results['f-score']}\"\n",
    "                )\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating the Progress of Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(tokenizer,textcat,test_data:list) -> dict:\n",
    "    reviews, labels = zip(*test_data)\n",
    "    reviews = (tokenizer(review) for review in reviews)\n",
    "    true_positives = 0\n",
    "    false_positives = 1e-8 #can't be 0 because of the presence in denominator\n",
    "    true_negatives = 0\n",
    "    false_negatives = 1e-8\n",
    "    \n",
    "    for i,review in enumerate(textcat.pipe(reviews)):\n",
    "        true_label = labels[i]['cats']\n",
    "        for predicted_label, score in review.cats.items():\n",
    "        #every category's dictionary includes both labels. You can get all the info you need with just the positive label\n",
    "            if predicted_label == \"neg\":\n",
    "                continue\n",
    "\n",
    "            if score >= 0.5 and true_label[\"pos\"]:\n",
    "                true_positives +=1\n",
    "            elif score >= 0.5 and true_label[\"neg\"]:\n",
    "                false_positives += 1\n",
    "            elif score < 0.5 and true_label[\"neg\"]:\n",
    "                true_negatives +=1\n",
    "            elif score < 0.5 and true_label[\"pos\"]:\n",
    "                false_negatives +=1\n",
    "    precision = true_positives / (true_positives + false_positives)\n",
    "    recall = true_positives / (true_positives + false_negatives)\n",
    "    \n",
    "    if precision + recall == 0:\n",
    "        f_score = 0\n",
    "    else:\n",
    "        f_score = 2 * (precision * recall) / (precision + recall)\n",
    "    return {\"precision\": precision, \"recall\":recall,\"f-score\":f_score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import spacy\n",
    "from spacy.util import minibatch, compounding\n",
    "\n",
    "def train_model(training_data: list, test_data: list, iterations: int= 20) -> None:\n",
    "    #Build Pipeline\n",
    "    nlp = spacy.load('en_core_web_sm') # load the english model\n",
    "    if \"textcat\" not in nlp.pipe_names:\n",
    "        textcat = nlp.create_pipe(\"textcat\", config = {\"architecture\": \"simple_cnn\"})\n",
    "        nlp.add_pipe(textcat,last=True)\n",
    "    else:\n",
    "        textcat = nlp.get_pipe('textcat')\n",
    "    \n",
    "    textcat.add_label(\"pos\")\n",
    "    textcat.add_label('neg')\n",
    "    \n",
    "    #Train only textcat\n",
    "    training_excluded_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"textcat\"]\n",
    "    \n",
    "    with nlp.disable_pipes(training_excluded_pipes):\n",
    "        optimizer = nlp.begin_training()\n",
    "        # Training loop\n",
    "        print(\"Beginning Training\")\n",
    "        print(\"Loss\\tPrecision\\tRecall\\tF-score\")\n",
    "        batch_sizes = compounding(4.0,32.0,1.001) # a generator that yields infinite series of input numbers\n",
    "        \n",
    "        for i in range(iterations):\n",
    "            loss = {}\n",
    "            random.shuffle(training_data)\n",
    "            batches = minibatch(training_data,size = batch_sizes)\n",
    "            for batch in batches:\n",
    "                text, labels = zip(*batch)\n",
    "                nlp.update(\n",
    "                    text,\n",
    "                    labels,\n",
    "                    drop = 0.2,\n",
    "                    sgd = optimizer,\n",
    "                    losses = loss\n",
    "                )\n",
    "            with textcat.model.use_params(optimizer.averages):\n",
    "                evaluation_results = evaluate_model(tokenizer = nlp.tokenizer,textcat=textcat,test_data = test_data) #evaluate model function\n",
    "                \n",
    "                print(\n",
    "                    f\"{loss['textcat']}\\t{evaluation_results['precision']}\"\n",
    "                    f\"\\t{evaluation_results['recall']}\"\n",
    "                    f\"\\t{evaluation_results['f-score']}\"\n",
    "                )\n",
    "    #Save model\n",
    "    with nlp.use_params(optimizer.averages):\n",
    "        nlp.to_disk(\"model_artifacts\")\n",
    "        \n",
    "    allDone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_REVIEW = ''\n",
    "\n",
    "def test_model(input_data: str = TEST_REVIEW):\n",
    "    #  Load saved trained model\n",
    "    loaded_model = spacy.load(\"model_artifacts\")\n",
    "    # Generate prediction\n",
    "    parsed_text = loaded_model(input_data)\n",
    "    # Determine prediction to return\n",
    "    if parsed_text.cats[\"pos\"] > parsed_text.cats[\"neg\"]:\n",
    "        prediction = \"Positive\"\n",
    "        score = parsed_text.cats[\"pos\"]\n",
    "    else:\n",
    "        prediction = \"Negative\"\n",
    "        score = parsed_text.cats[\"neg\"]\n",
    "    print(\n",
    "        f\"Review text: {input_data}\\nPredicted sentiment: {prediction}\"\n",
    "        f\"\\tScore: {score}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step with this new function will be to load the previously saved model. While you could use the model in memory, loading the saved model artifact allows you to optionally skip training altogether, which you’ll see later. Here’s the test_model() signature along with the code to load your saved model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = load_training_data(limit = 2500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning Training\n",
      "Loss\tPrecision\tRecall\tF-score\n",
      "11.092800595564768\t0.7715517241046744\t0.7458333333022569\t0.7584745762390477\n",
      "1.9931817497708835\t0.8251121075863178\t0.7666666666347222\t0.7948164146524916\n",
      "0.5325804846115716\t0.8035714285355549\t0.74999999996875\t0.775862068932075\n",
      "0.202051523645423\t0.8156682027273885\t0.7374999999692708\t0.7746170677997979\n",
      "0.07674300500184472\t0.8165137614304351\t0.7416666666357639\t0.7772925763852709\n",
      "0.03717643609240895\t0.8224299065036247\t0.7333333333027777\t0.7753303964416154\n",
      "0.017342102308703033\t0.827102803699668\t0.7374999999692708\t0.7797356827850337\n",
      "0.017334968310365184\t0.8279069767056787\t0.7416666666357639\t0.7824175823831905\n",
      "0.006170690562953496\t0.8287037036653377\t0.7458333333022569\t0.7850877192638118\n",
      "0.003991205511340468\t0.8254716980742701\t0.7291666666362847\t0.774336283151578\n",
      "0.002378680516658793\t0.8301886792061232\t0.7333333333027777\t0.7787610619124441\n",
      "0.0017550996148685272\t0.8262910797734135\t0.7333333333027777\t0.7770419425705499\n",
      "0.0012567566650076856\t0.8262910797734135\t0.7333333333027777\t0.7770419425705499\n",
      "0.0007906284599243918\t0.827102803699668\t0.7374999999692708\t0.7797356827850337\n",
      "0.0007960796682695559\t0.818604651124716\t0.7333333333027777\t0.773626373592368\n",
      "0.0009365633898212877\t0.8194444444065072\t0.7374999999692708\t0.7763157894396352\n",
      "0.0007546237825337698\t0.827102803699668\t0.7374999999692708\t0.7797356827850337\n",
      "0.0007054702925657352\t0.8254716980742701\t0.7291666666362847\t0.774336283151578\n",
      "0.0006193281176969379\t0.8246445497239505\t0.7249999999697917\t0.7716186252429437\n",
      "0.000639101445273127\t0.8246445497239505\t0.7249999999697917\t0.7716186252429437\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                <audio  controls=\"controls\" autoplay=\"autoplay\">\n",
       "                    <source src=\"https://sound.peal.io/ps/audios/000/000/537/original/woo_vu_luvub_dub_dub.wav\" type=\"audio/wav\" />\n",
       "                    Your browser does not support the audio element.\n",
       "                </audio>\n",
       "              "
      ],
      "text/plain": [
       "<IPython.lib.display.Audio object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_model(train,test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review text: An odd thought occurred to me a few hours after I saw writer/director Wes Anderson's \"The Grand Budapest Hotel\" for the first time. It was that Anderson would be the ideal director for a film of \"Lolita,\" or a mini-series of \"Ada.\" Now I know that \"Lolita\" has been filmed, twice, but the fundamental problem with each version has nothing to do with ability to depict or handle risky content but with a fundamental misapprehension that Nabokov's famous novel took place in the \"real world.\" For all the authentic horror and tragedy of its story, it does not. \"I am thinking of aurochs and angels, the secret of durable pigments, prophetic sonnets, the refuge of art,\" Humbert Humbert, the book's monstrous protagonist/narrator, writes at the end of \"Lolita.\" Nabokov created Humbert so Humbert might create his own world (with a combination of detail both geographically verifiable and stealthily fanciful), a refuge from his own wrongdoing.\n",
      "\n",
      "\"The Grand Budapest Hotel\" uses a not dissimilar narrative stratagem, a nesting-doll contrivance conveyed in a blink-and-you'll-miss-a-crucial-part-of-it opening. A young lady visits a park and gazes at a bust of a beloved \"Author,\" who is then made flesh in the person of Tom Wilkinson, who then recalls his younger self in the person of Jude Law, who then recounts his meeting with Mr. Moustafa (F. Murray Abraham), the owner of the title hotel. Said hotel is a legendary edifice falling into obsolescence, and Law's \"Author\" is curious as to why the immensely wealthy Moustafa chooses to bunk in a practically closet-size room on his yearly visits to the place. Over dinner. Moustafa deigns to satisfy the writer's curiosity, telling him of his apprenticeship under the hotel's one-time concierge, M. Gustave (Ralph Fiennes).\n",
      "\n",
      "All of this material is conveyed not just in the standard Wes Anderson style, e.g., meticulously composed and designed shots with precise and very constricted camera movements. In \"Hotel\" Anderson's refinement of his particular moviemaking mode is so distinct that his debut feature, the hardly unstylized \"Bottle Rocket,\" looks like a Cassavetes picture by comparison. So, to answer some folks who claim to enjoy Anderson's movies while also grousing that they wish he would apply his cinematic talents in a \"different\" mode: no, this isn't the movie in which he does what you think you want, whatever that is.\n",
      "\n",
      "What he does is his own thing, which in terms of achievement is on a similar level of difficulty to what Nabokov kept upping the ante on in his English-language novels: to conjure poignancy and tragedy in the context of realms spun off from but also fancifully, madly removed from dirt-under-your-fingernails \"reality.\" M. Gustave is a didact of high-level service, schooling young Zero Moustafa in the art of understanding what a guest wants, and getting it to the guest, before the guest has even thought of it. He wears a scent called \"Eau de Panache.\" He's also a ludicrous horndog and gigolo, and his troubles begin when the wealthiest of his dowagers (Tilda Swinton) dies and leaves him a strange painting. The dowager's impossibly evil son (Adrian Brody) wishes M. Gustave to get nothing, and will stop at nothing to see to that. His determination sets into motion a series of intimidations and assaults that's complicated by the rise of an ostensibly Fascist power in the often-candy-colored Middle-Europe Bohemian Theme Park Anderson and his production designers conjure up here. (Since I've invoked Nabokov twice in this review, I really ought to emphasize that the movie itself credits the writings of Stefan Zweig, the Austrian writer whose wry, poignant autobiography was titled \"The World of Yesterday,\" as a primary inspiration.)\n",
      "\n",
      "The dialogue is contemporary American, with plenty of cursing; the action is often grisly slapstick, with an upping of the imperiled-animal quotient that provided one of the more disquieting scenes of Anderson's last feature, \"Moonrise Kingdom.\" The references are multitudinous, and come from everywhere (one of my favorites is a cable car sequence nodding to Carol Reed's 1940 thriller \"Night Train To Munich\"). The cast is the usual top-to-bottom array of incredible talent, including, aside from the aforementioned, Matthew Amalric, Willem Dafoe, Jeff Goldblum, Harvey Keitel, Edward Norton, Saoirse Ronan, Léa Seydoux, and Anderson stalwarts Bill Murray, Jason Schwartzman, and Owen Wilson. (Newcomer Tony Revolori plays the young Moustafa.) The settings include not just the hotel but also a dank prison, a heavenly bakeshop, and all manner of horse drawn or steam-driven conveyances.\n",
      "\n",
      "Although it's packed with incident, there's a stillness to the film that makes looking at it feel as if you're staring at a zoetrope image of a snow globe, while at the same time a stray epithet here or the spectacle of some severed digits there pulls in a different direction, suggesting Anderson's conjured world is subject to tensions that exist entirely outside of it, calling attention to that which is unseen on the screen: an anxious creator who wants everything just so, but can't control the intrusion of vulgarity or cruelty. This tension is reflected in the character of M. Gustave himself, whose air of refinement masks a boyish exuberance and vulgarity, and who is nevertheless revealed at the movie's end to be a human being of absolute nobility.\n",
      "\n",
      "As much as \"The Grand Budapest Hotel\" takes on the aspect of a cinematic confection, it does so to grapple with the very raw and, yes, real stuff of humanity from an unusual but highly illuminating angle. \"The Grand Budapest Hotel\" is a movie about the masks we conjure to suit our aspirations, and the cost of keeping up appearances. \"He certainly maintained the illusion with remarkable grace,\" one character remarks admiringly of another near the end of the movie. \"The Grand Budapest Hotel\" suggests that sometimes, as a species, that's the best we can do. Anderson the illusion-maker is more than graceful, he's dazzling, and with this movie he's created an art-refuge that consoles and commiserates. It's an illusion, but it's not a lie.\n",
      "Predicted sentiment: Positive\tScore: 0.5738075971603394\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import spacy\n",
    "from spacy.util import minibatch, compounding\n",
    "\n",
    "\n",
    "\n",
    "test_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sentiment",
   "language": "python",
   "name": "sentiment"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
